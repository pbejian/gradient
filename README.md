# Descente de gradient pour une rÃ©gression linÃ©aire (univariÃ©e)

L'application est exÃ©cutable en ligne Ã  l'adresse suivante :

ğŸš€ [https://https://pbejian-gradient-gradient-uiu2uv.streamlitapp.com/](https://pbejian-gradient-gradient-uiu2uv.streamlitapp.com/).


On travaille sur une rÃ©gression linÃ©aire dont les valeurs d'entrainement sont les suivantes :

$$
X = (1.081, 1.854, 2.674, 3.753, 4.693, 5.498, 6.470, 7.386, 7.981, 9.101)
$$

$$
Y = (3.165, 6.047, 4.831, 8.790, 9.266, 14.059, 17.403, 21.370, 21.400, 27.870)
$$


La ***fonction de coÃ»t*** que l'on doit minimiser est la suivante :

$$
J(w,b) = \dfrac{1}{m} \sum_{k=1}^m  \big((wx_i + b) - y_i\big) ^2
$$


On cherche un modÃ¨le de la forme :

$$
f(x) = wx + b
$$

oÃ¹ les coefficients $w$ et $b$ sont les valeurs qui minimisent la fonction de coÃ»t. Pour calculer ces deux coefficients on utilise l'algorithme de ***descente de gradient*** qui consiste Ã  rÃ©pÃ©ter les calculs suivants (boucle $\texttt{for}$ ou $\texttt{while}$) :

$$
w := w -\alpha \dfrac{\partial J}{\partial w}(w,b) \\\\
$$

$$
b := b -\alpha \dfrac{\partial J}{\partial b}(w,b)
$$

oÃ¹ $\alpha$ est un paramÃ¨tre appelÃ© ***learning rate***. Le nombre d'itÃ©ration ainsi que $\alpha$ sont des Â« hyperparamÃ¨tres Â» qu'il convient de choisir judicieusement (c'est Ã  dire ?). Il faut aussi donner une valeurs initiales Ã  $w$ et $b$. Sauf raison particuliÃ¨re, on peut prendre $w =0$ et $b=0$.  

Voici la dÃ©rivÃ©e partielle de $J$ par rapport Ã  $w$ :

$$
\dfrac{\partial J}{\partial w}(w,b) = \frac{1}{m} \sum_{k=1}^m  2\big((wx_i + b) - y_i\big) \times x_i
$$

Puis celle par rapport Ã  $b$ :

$$
\dfrac{\partial J}{\partial w}(w,b) = \frac{1}{m} \sum_{k=1}^m  2\big((wx_i + b) - y_i\big)
$$


**Remarque --** Dans cette version de l'algorithme de descente de gradient on utilise plusieurs boucles $\texttt{for}$. Pour Ãªtre efficace si l'on a beaucoup de donnÃ©es, il serait judicieux de Â« vectoriser Â» les calculs.